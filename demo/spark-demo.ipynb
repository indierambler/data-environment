{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark and Hadoop/HDFS Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages to venv\n",
    "%pip install pandas\n",
    "%pip install hdfs\n",
    "%pip install pyspark pyspark[sql]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "from pyspark.sql import SparkSession, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set inputs and outputs for pipes\n",
    "HDFS_SERVER = 'x.x.x.x'  # IP address of the machine where the HDFS and Spark containers are run\n",
    "HDFS_IPC_PORT = '9000'\n",
    "HDFS_HTTP_PORT = '9870'\n",
    "HDFS_USER = 'root'\n",
    "HDFS_IPC_URL = f'hdfs://{HDFS_SERVER}:{HDFS_IPC_PORT}'  # pyspark connector port\n",
    "HDFS_HTTP_URL = f'http://{HDFS_SERVER}:{HDFS_HTTP_PORT}'  # accesses the http port directly (bypasses reverse proxy, doesn't work if reverse proxy is used)\n",
    "HDFS_HTTP_URL2 = 'http://hadoop-subdomain.domain.com'  # accesses the web server through the reverse proxy\n",
    "hdfs_user_dir = f'/user/{HDFS_USER}'\n",
    "hdfs_test_file = 'df_small.csv'\n",
    "local_test_dir = f'/home/{os.getenv(\"USER\")}/data-environment/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open/Create HDFS client\n",
    "hdfs_client = InsecureClient(HDFS_HTTP_URL, user=HDFS_USER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HDFS directory\n",
    "hdfs_path = os.path.join(hdfs_user_dir, 'demo')\n",
    "hdfs_client.makedirs(hdfs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List HDFS files and directories\n",
    "contents = hdfs_client.list(hdfs_user_dir)\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if HDFS file or directory exists\n",
    "def is_hdfs_path(path, hdfs_client):\n",
    "    head, tail = os.path.split(path)\n",
    "    # check that head path exists\n",
    "    try:\n",
    "        contents = hdfs_client.list(head)\n",
    "    except:\n",
    "        return False\n",
    "    # check if tail file or dir in head path\n",
    "    if tail in contents:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "test_path = os.path.join(hdfs_user_dir, hdfs_test_file)\n",
    "print(is_hdfs_path(test_path, hdfs_client))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**  \n",
    "In order to download and upload to a remote HDFS, the hadoop datanode needs to be added to your local hosts  \n",
    "To do this run the following on the command line:  \n",
    "```echo \"x.x.x.x hadoop-datanode\" >> /etc/hosts```  \n",
    "where x.x.x.x is your server's IP address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from HDFS (NOT WORKING: local machine and namenode both need to resolve the datanode hostname which may not be possible using docker)\n",
    "hdfs_path = os.path.join(hdfs_user_dir, hdfs_test_file)\n",
    "local_path = os.path.join(local_test_dir, hdfs_test_file)\n",
    "hdfs_client.download(hdfs_path, local_path, n_threads=1, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file to HDFS (NOT WORKING: local machine and namenode both need to resolve the datanode hostname which may not be possible using docker)\n",
    "local_path = os.path.join(local_test_dir, hdfs_test_file)\n",
    "hdfs_path = os.path.join(hdfs_user_dir, hdfs_test_file)\n",
    "hdfs_client.upload(hdfs_path, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete an HDFS directory\n",
    "hdfs_path = os.path.join(hdfs_user_dir, 'test')\n",
    "hdfs_client.delete(hdfs_path, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration params and input/output files and directories\n",
    "JAVA_HOME = '/usr/lib/jvm/java-21-openjdk'  # this final directory may change after system updates\n",
    "SPARK_SERVER = 'x.x.x.x'  # IP address of machine running spark container (set to \"local[n]\" to run on local spark instance with n cores)\n",
    "SPARK_MASTER_PORT = '7077'\n",
    "SPARK_CONNECT_PORT = '15002'\n",
    "SPARK_MASTER_URL = f'spark://{SPARK_SERVER}:{SPARK_MASTER_PORT}'\n",
    "SPARK_CONNECT_URL = f'sc://{SPARK_SERVER}:{SPARK_CONNECT_PORT}'\n",
    "hdfs_user_dir = f'/user/{HDFS_USER}'\n",
    "hdfs_test_dir = os.path.join(hdfs_user_dir, 'demo')\n",
    "local_test_dir = f'/home/{os.getenv(\"USER\")}/data-environment/test'\n",
    "test_filename = 'df_small'\n",
    "test_csv_file = f'{test_filename}.csv'\n",
    "test_parquet_file = f'{test_filename}.parquet'\n",
    "\n",
    "# set JAVA_HOME env variable\n",
    "os.system(f'export JAVA_HOME={JAVA_HOME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a spark-connect session (from machines where local is accessing a remote spark instance)\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"connect-test-pipe\") \\\n",
    "  .remote(SPARK_CONNECT_URL) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to a direct spark session (from machines where local is connected and running a spark instance)\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"master-test-pipe\") \\\n",
    "  .master(SPARK_MASTER_URL) \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark dataframe with explicit schema\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe to csv file\n",
    "remote_path = os.path.join(hdfs_test_dir, test_csv_file)\n",
    "#df.write.mode(\"overwrite\").csv(remote_path), header=True)  # set to overwrite if already exists\n",
    "df.write.csv(remote_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataframe from the csv file (all values appear to read in as string type)\n",
    "remote_path = os.path.join(hdfs_test_dir, test_csv_file)\n",
    "df_csv = spark.read.csv(remote_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dataframe to parquet\n",
    "remote_path = os.path.join(hdfs_test_dir, test_parquet_file)\n",
    "df_csv.write.mode(\"overwrite\").parquet(remote_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the csv to local file system\n",
    "local_path = os.path.join(local_test_dir, test_csv_file)\n",
    "df_pandas = df.toPandas()\n",
    "df_pandas.to_csv(local_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
